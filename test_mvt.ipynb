{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01720e34-eb55-49a1-831b-3997dc483d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/constantin/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/constantin/miniconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from helpers import plotframes, reconstruct_from_patches, plotframes_tensorboard\n",
    "from mvt import MaskedVideoTransformer\n",
    "from gameds import GameDS\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c27012b-40d6-4997-a06c-af6ff8d81a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3195debf-d31b-4b65-924f-6e0aad6f98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_ds = GameDS(\"/tmp/sequences\", train=True)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=24, shuffle=True, prefetch_factor=4, num_workers=4)\n",
    "val_ds = GameDS(\"/tmp/sequences\", train=False)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=24, shuffle=True, prefetch_factor=4, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cda2a88-90f3-43a9-bdc8-eebd16985e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedVideoTransformer()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7396ca1c-a0ad-4f72-947a-2b1674feffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mconstantin123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/constantin/Documents/maskedvideoencoder/wandb/run-20231031_185142-95cz8auc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/constantin123/my_project/runs/95cz8auc' target=\"_blank\">abominable-newt-8</a></strong> to <a href='https://wandb.ai/constantin123/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/constantin123/my_project' target=\"_blank\">https://wandb.ai/constantin123/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/constantin123/my_project/runs/95cz8auc' target=\"_blank\">https://wandb.ai/constantin123/my_project/runs/95cz8auc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class DualLogger:\n",
    "    def __init__(self, log_dir=None, project_name=None):\n",
    "        \"\"\"\n",
    "        log_dir: Directory for TensorBoard logs\n",
    "        project_name: Weights & Biases project name\n",
    "        \"\"\"\n",
    "        self.tensorboard_writer = SummaryWriter(log_dir=log_dir)\n",
    "        \n",
    "        if project_name:\n",
    "            wandb.init(project=project_name)\n",
    "    \n",
    "    def add_scalar(self, tag, scalar_value, global_step=None):\n",
    "        # Logging to TensorBoard\n",
    "        self.tensorboard_writer.add_scalar(tag, scalar_value, global_step)\n",
    "\n",
    "        # Logging to wandb\n",
    "        wandb.log({tag: scalar_value}, step=global_step)\n",
    "    \n",
    "    def add_image(self, tag, img_tensor, global_step=None):\n",
    "        # Logging to TensorBoard\n",
    "        self.tensorboard_writer.add_image(tag, img_tensor, global_step)\n",
    "\n",
    "        # Logging to wandb. Note: wandb requires image data to be in PIL or numpy format.\n",
    "        # Assuming img_tensor is a PyTorch tensor, we can convert it to a wandb-compatible format.\n",
    "        image = wandb.Image(img_tensor.permute(1, 2, 0).cpu().numpy())  # Assuming img_tensor is of shape (C, H, W)\n",
    "        wandb.log({tag: image}, step=global_step)\n",
    "    \n",
    "    def close(self):\n",
    "        self.tensorboard_writer.close()\n",
    "        wandb.finish()\n",
    "\n",
    "# Usage:\n",
    "writer = DualLogger(log_dir='./logs', project_name='my_project')\n",
    "#writer.add_scalar('Training loss', 0.5, 1)\n",
    "#writer.add_image('Train: Mask', image_tensor, 1)  # image_tensor should be a PyTorch tensor.\n",
    "#writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d31e611-7e1f-4fb7-9ac3-3b6eee230972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = SummaryWriter(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706bde19-bcb3-4b11-ba72-748dce0f91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983597e3-e7af-443d-82a2-a004fe07c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20156dc0-6052-4415-a3fd-d4db617cd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_to_canvas(image, canvas_size=(256, 256)):\n",
    "    # Get the aspect ratio of the image\n",
    "    aspect_ratio = image.width / image.height\n",
    "\n",
    "    if image.width > image.height:  # Landscape or square\n",
    "        new_width = canvas_size[0]\n",
    "        new_height = int(canvas_size[0] / aspect_ratio)\n",
    "    else:  # Portrait\n",
    "        new_height = canvas_size[1]\n",
    "        new_width = int(canvas_size[1] * aspect_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    # Create a new blank canvas\n",
    "    canvas = Image.new(\"RGB\", canvas_size, \"white\")\n",
    "\n",
    "    # Compute the position to paste the resized image onto the canvas\n",
    "    x_offset = (canvas_size[0] - new_width) // 2\n",
    "    y_offset = (canvas_size[1] - new_height) // 2\n",
    "\n",
    "    canvas.paste(resized_image, (x_offset, y_offset))\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad76db9f-9706-4905-ab16-80e266c3dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "class HelaData:\n",
    "\n",
    "    def __init__(self, base_dir=\"train\", sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = []\n",
    "        self.sequence_start_indices = []\n",
    "        self.current_start_index = 0\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        for burst in os.listdir(base_dir):\n",
    "            burst_path = os.path.join(base_dir, burst, \"img1\")\n",
    "            frames = sorted(os.listdir(burst_path))\n",
    "            frames = [os.path.join(base_dir, burst, \"img1\", x) for x in frames]\n",
    "            self.sequences.append(frames)\n",
    "            self.sequence_start_indices.append(self.current_start_index)\n",
    "            self.current_start_index += len(frames) - self.sequence_length\n",
    "        #print(self.sequence_start_indices)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(max([x for x in self.sequence_start_indices if x <= idx]))\n",
    "        start_item = max([x for x in self.sequence_start_indices if x <= idx])\n",
    "        start_index = self.sequence_start_indices.index(start_item)\n",
    "        \n",
    "        local_index = idx - start_item\n",
    "        files = self.sequences[start_index][local_index:local_index+self.sequence_length]\n",
    "        \n",
    "        images = [resize_to_canvas(Image.open(f)) for f in files]\n",
    "        tensors = [torch.tensor(np.array(im)) for im in images]\n",
    "        stacked = torch.stack(tensors)\n",
    "        result = stacked\n",
    "        result = result.moveaxis(3, 1)[:, :, :336, :464]\n",
    "        return result, torch.Tensor([0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.current_start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6742728-d5c0-4a03-bc08-e8adfcce9fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhela_train = HelaData(\"/home/constantin/celltracking/HeLa_dataset/train\")\\nhela_val = HelaData(\"/home/constantin/celltracking/HeLa_dataset/test\")\\ntrain_dataloader = DataLoader(hela_train, batch_size=24, shuffle=True, prefetch_factor=4, num_workers=4)\\nval_dataloader = DataLoader(hela_val, batch_size=24, shuffle=True, prefetch_factor=4, num_workers=4)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hela_train = HelaData(\"/home/constantin/celltracking/HeLa_dataset/train\")\n",
    "hela_val = HelaData(\"/home/constantin/celltracking/HeLa_dataset/test\")\n",
    "train_dataloader = DataLoader(hela_train, batch_size=24, shuffle=True, prefetch_factor=4, num_workers=4)\n",
    "val_dataloader = DataLoader(hela_val, batch_size=24, shuffle=True, prefetch_factor=4, num_workers=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d9a78-4cbb-4798-9cb9-0e4e18b9625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "\n",
    "step = 0\n",
    "for _ in range(1000):\n",
    "    for i_step, (X, _) in enumerate(train_dataloader):\n",
    "        X = X / 255\n",
    "        X = X.to(device)\n",
    "        \n",
    "        \n",
    "        X_pred, (X_masked, ) = model(X)\n",
    "        \n",
    "        loss = criterion(X_pred, X)\n",
    "    \n",
    "        loss.backward()\n",
    "        if step%20 == 1:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        writer.add_scalar('Training loss', loss.item(), step)\n",
    "    \n",
    "        if step%100 == 1:\n",
    "            image_tensor = plotframes_tensorboard(X_masked, f\"Train: Mask {step}\")\n",
    "            writer.add_image('Train: Mask', image_tensor, step)\n",
    "            \n",
    "            image_tensor = plotframes_tensorboard(X_pred, f\"Train: Prediction {step}\")\n",
    "            writer.add_image('Train: Prediction', image_tensor, step)\n",
    "        \n",
    "            image_tensor = plotframes_tensorboard(X, f\"Train: Original {step}\")\n",
    "            writer.add_image('Train: Original', image_tensor, step)\n",
    "    \n",
    "            # evaluation dataset\n",
    "            X_eval, _ = random.choice(val_ds)\n",
    "            X_eval = X_eval / 255\n",
    "            X_eval = X_eval[None].to(device)\n",
    "            X_eval_pred, (X_eval_pred_masked, ) = model(X_eval)\n",
    "\n",
    "            loss = criterion(X_eval_pred, X_eval)\n",
    "            writer.add_scalar('Validation loss', loss.item(), step)\n",
    "    \n",
    "            image_tensor = plotframes_tensorboard(X_eval_pred_masked, f\"Val: Mask {step}\")\n",
    "            writer.add_image('Val: Mask', image_tensor, step)\n",
    "            \n",
    "            image_tensor = plotframes_tensorboard(X_eval_pred, f\"Val: Prediction {step}\")\n",
    "            writer.add_image('Val: Prediction', image_tensor, step)\n",
    "        \n",
    "            image_tensor = plotframes_tensorboard(X_eval, f\"Val: Original {step}\")\n",
    "            writer.add_image('Val: Original', image_tensor, step)\n",
    "\n",
    "            del X_eval_pred\n",
    "            del X_eval_pred_masked\n",
    "        \n",
    "        del X_pred\n",
    "        del X_masked\n",
    "        step += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185e02d-041e-4288-9d2c-176099d94cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
